<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>hadoop tutorial | Mengzhuo You</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="核心功能描述应用程序通常会通过提供map和reduce来实现 Mapper和Reducer接口，它们组成作业的核心。 MapperMapper将输入键值对(key/value pair)映射到一组中间格式的键值对集合。Map是一类将输入记录集转换为中间格式记录集的独立任务。 Hadoop Map/Reduce框架为每一个InputSplit产生一个map任务，而每个InputSplit是由该作业的">
<meta name="keywords" content="hadoop">
<meta property="og:type" content="article">
<meta property="og:title" content="hadoop tutorial">
<meta property="og:url" content="http://yoursite.com/2018/03/18/hadoop-tutorial/index.html">
<meta property="og:site_name" content="Mengzhuo You">
<meta property="og:description" content="核心功能描述应用程序通常会通过提供map和reduce来实现 Mapper和Reducer接口，它们组成作业的核心。 MapperMapper将输入键值对(key/value pair)映射到一组中间格式的键值对集合。Map是一类将输入记录集转换为中间格式记录集的独立任务。 Hadoop Map/Reduce框架为每一个InputSplit产生一个map任务，而每个InputSplit是由该作业的">
<meta property="og:updated_time" content="2018-03-18T21:53:30.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="hadoop tutorial">
<meta name="twitter:description" content="核心功能描述应用程序通常会通过提供map和reduce来实现 Mapper和Reducer接口，它们组成作业的核心。 MapperMapper将输入键值对(key/value pair)映射到一组中间格式的键值对集合。Map是一类将输入记录集转换为中间格式记录集的独立任务。 Hadoop Map/Reduce框架为每一个InputSplit产生一个map任务，而每个InputSplit是由该作业的">
  
    <link rel="alternate" href="/atom.xml" title="Mengzhuo You" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Mengzhuo You</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="Flux RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Rechercher"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-hadoop-tutorial" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/18/hadoop-tutorial/" class="article-date">
  <time datetime="2018-03-18T21:22:38.000Z" itemprop="datePublished">2018-03-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      hadoop tutorial
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="核心功能描述"><a href="#核心功能描述" class="headerlink" title="核心功能描述"></a>核心功能描述</h2><p>应用程序通常会通过提供map和reduce来实现 Mapper和Reducer接口，它们组成作业的核心。</p>
<h3 id="Mapper"><a href="#Mapper" class="headerlink" title="Mapper"></a>Mapper</h3><p>Mapper将输入键值对(key/value pair)映射到一组中间格式的键值对集合。<br>Map是一类将输入记录集转换为中间格式记录集的独立任务。</p>
<p>Hadoop Map/Reduce框架为每一个InputSplit产生一个map任务，而每个InputSplit是由该作业的InputFormat产生的。<br>概括地说，对Mapper的实现需要重写 JobConfigurable.configure(JobConf)方法，这个方法需要传递一个JobConf参数，目的是完成Mapper的初始化工作。<br>然后，框架为这个任务的InputSplit中每个键值对调用一次 map(WritableComparable, Writable, OutputCollector, Reporter)操作。</p>
<p>通过调用 OutputCollector.collect(WritableComparable,Writable)可以收集输出的键值对。</p>
<p>应用程序可以使用Reporter报告进度，设定应用级别的状态消息，更新Counters（计数器），或者仅是表明自己运行正常。</p>
<p>框架随后会把与一个特定key关联的所有中间过程的值（value）分成组，然后把它们传给Reducer以产出最终的结果。用户可以通过 JobConf.setOutputKeyComparatorClass(Class)<br>来指定具体负责分组的 Comparator。</p>
<p>Mapper的输出被排序后，就被划分给每个Reducer。分块的总数目和一个作业的reduce任务的数目是一样的。用户可以通过实现自定义的 Partitioner来控制哪个key被分配给哪个 Reducer。</p>
<p>用户可选择通过 JobConf.setCombinerClass(Class)指定一个combiner，它负责对中间过程的输出进行本地的聚集，这会有助于降低从Mapper到 Reducer数据传输量。</p>
<h4 id="需要多少个Map？"><a href="#需要多少个Map？" class="headerlink" title="需要多少个Map？"></a>需要多少个Map？</h4><p>Map的数目通常是由输入数据的大小决定的，一般就是所有输入文件的总块（block）数。</p>
<p>Map正常的并行规模大致是每个节点（node）大约10到100个map，对于CPU 消耗较小的map任务可以设到300个左右。由于每个任务初始化需要一定的时间，因此，比较合理的情况是map执行的时间至少超过1分钟。</p>
<p>这样，如果你输入10TB的数据，每个块（block）的大小是128MB，你将需要大约82,000个map来完成任务，除非使用 setNumMapTasks(int)（注意：这里仅仅是对框架进行了一个提示(hint)，实际决定因素见这里）将这个数值设置得更高。</p>
<h3 id="Reducer"><a href="#Reducer" class="headerlink" title="Reducer"></a>Reducer</h3><p>Reducer将与一个key关联的一组中间数值集归约（reduce）为一个更小的数值集。</p>
<p>概括地说，对Reducer的实现者需要重写 JobConfigurable.configure(JobConf) 方法，这个方法需要传递一个JobConf参数，目的是完成Reducer的初始化工作。然后，框架为成组的输入数据中的每个<key, (list="" of="" values)="">对调用一次 reduce(WritableComparable, Iterator, OutputCollector, Reporter)方法。</key,></p>
<p>Reducer有3个主要阶段：shuffle、sort和reduce。</p>
<p>Shuffle<br>Reducer的输入就是Mapper已经排好序的输出。在这个阶段，框架通过HTTP为每个Reducer获得所有Mapper输出中与之相关的分块。</p>
<p>Sort<br>这个阶段，框架将按照key的值对Reducer的输入进行分组 （因为不同mapper的输出中可能会有相同的key）。</p>
<p>Shuffle和Sort两个阶段是同时进行的；map的输出也是一边被取回一边被合并的。</p>
<p>Secondary Sort<br>如果需要中间过程对key的分组规则和reduce前对key的分组规则不同，那么可以通过 JobConf.setOutputValueGroupingComparator(Class)来指定一个Comparator。再加上 JobConf.setOutputKeyComparatorClass(Class)可用于控制中间过程的key如何被分组，所以结合两者可以实现按值的二次排序。</p>
<p>Reduce<br>在这个阶段，框架为已分组的输入数据中的每个 <key, (list="" of="" values)="">对调用一次 reduce(WritableComparable, Iterator, OutputCollector, Reporter)方法。</key,></p>
<p>Reduce任务的输出通常是通过调用 OutputCollector.collect(WritableComparable, Writable)写入 文件系统的。</p>
<p>应用程序可以使用Reporter报告进度，设定应用程序级别的状态消息，更新Counters（计数器），或者仅是表明自己运行正常。</p>
<p>Reducer的输出是没有排序的。</p>
<h4 id="需要多少个Reduce？"><a href="#需要多少个Reduce？" class="headerlink" title="需要多少个Reduce？"></a>需要多少个Reduce？</h4><p>Reduce的数目建议是0.95或1.75乘以 (<no. of="" nodes=""> * mapred.tasktracker.reduce.tasks.maximum)。</no.></p>
<p>用0.95，所有reduce可以在maps一完成时就立刻启动，开始传输map的输出结果。用1.75，速度快的节点可以在完成第一轮reduce任务后，可以开始第二轮，这样可以得到比较好的负载均衡的效果。</p>
<p>增加reduce的数目会增加整个框架的开销，但可以改善负载均衡，降低由于执行失败带来的负面影响。</p>
<p>上述比例因子比整体数目稍小一些是为了给框架中的推测性任务（speculative-tasks） 或失败的任务预留一些reduce的资源。</p>
<h4 id="无Reducer"><a href="#无Reducer" class="headerlink" title="无Reducer"></a>无Reducer</h4><p>如果没有归约要进行，那么设置reduce任务的数目为零是合法的。</p>
<p>这种情况下，map任务的输出会直接被写入由 setOutputPath(Path)指定的输出路径。框架在把它们写入FileSystem之前没有对它们进行排序。</p>
<h2 id="作业配置"><a href="#作业配置" class="headerlink" title="作业配置"></a>作业配置</h2><p>JobConf代表一个Map/Reduce作业的配置。</p>
<p>JobConf是用户向Hadoop框架描述一个Map/Reduce作业如何执行的主要接口。框架会按照JobConf描述的信息忠实地去尝试完成这个作业。</p>
<p>通常，JobConf会指明Mapper、Combiner(如果有的话)、 Partitioner、Reducer、InputFormat和 OutputFormat的具体实现。JobConf还能指定一组输入文件 (setInputPaths(JobConf, Path…) /addInputPath(JobConf, Path)) 和(setInputPaths(JobConf, String) /addInputPaths(JobConf, String)) 以及输出文件应该写在哪儿 (setOutputPath(Path))。</p>
<h2 id="作业的输入"><a href="#作业的输入" class="headerlink" title="作业的输入"></a>作业的输入</h2><p>InputFormat 为Map/Reduce作业描述输入的细节规范。</p>
<p>Map/Reduce框架根据作业的InputFormat来：</p>
<p>检查作业输入的有效性。<br>把输入文件切分成多个逻辑InputSplit实例， 并把每一实例分别分发给一个 Mapper。<br>提供RecordReader的实现，这个RecordReader从逻辑InputSplit中获得输入记录， 这些记录将由Mapper处理。<br>基于文件的InputFormat实现（通常是 FileInputFormat的子类） 默认行为是按照输入文件的字节大小，把输入数据切分成逻辑分块（logical InputSplit ）。    其中输入文件所在的FileSystem的数据块尺寸是分块大小的上限。下限可以设置mapred.min.split.size 的值。</p>
<p>考虑到边界情况，对于很多应用程序来说，很明显按照文件大小进行逻辑分割是不能满足需求的。 在这种情况下，应用程序需要实现一个RecordReader来处理记录的边界并为每个任务提供一个逻辑分块的面向记录的视图。</p>
<p>TextInputFormat 是默认的InputFormat。</p>
<p>如果一个作业的Inputformat是TextInputFormat， 并且框架检测到输入文件的后缀是.gz和.lzo，就会使用对应的CompressionCodec自动解压缩这些文件。 但是需要注意，上述带后缀的压缩文件不会被切分，并且整个压缩文件会分给一个mapper来处理。</p>
<p>InputSplit<br>InputSplit 是一个单独的Mapper要处理的数据块。</p>
<p>一般的InputSplit 是字节样式输入，然后由RecordReader处理并转化成记录样式。</p>
<p>FileSplit 是默认的InputSplit。 它把 map.input.file 设定为输入文件的路径，输入文件是逻辑分块文件。</p>
<p>RecordReader<br>RecordReader 从InputSlit读入<key, value="">对。</key,></p>
<p>一般的，RecordReader 把由InputSplit 提供的字节样式的输入文件，转化成由Mapper处理的记录样式的文件。 因此RecordReader负责处理记录的边界情况和把数据表示成keys/values对形式。</p>
<h2 id="作业的输出"><a href="#作业的输出" class="headerlink" title="作业的输出"></a>作业的输出</h2><p>OutputFormat 描述Map/Reduce作业的输出样式。</p>
<p>Map/Reduce框架根据作业的OutputFormat来：</p>
<p>检验作业的输出，例如检查输出路径是否已经存在。<br>提供一个RecordWriter的实现，用来输出作业结果。 输出文件保存在FileSystem上。<br>TextOutputFormat是默认的 OutputFormat。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/18/hadoop-tutorial/" data-id="cjf5w1wgt001imdf2xce4usaf" class="article-share-link">Partager</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">hadoop</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/03/21/Leetcode-045/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Récent</strong>
      <div class="article-nav-title">
        
          Leetcode-045
        
      </div>
    </a>
  
  
    <a href="/2018/03/06/Cracking-the-Interview/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Ancien</strong>
      <div class="article-nav-title">Cracking the Interview</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Mot-clés</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Big-Data-Process/">Big Data Process</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cracking-the-Interview/">Cracking the Interview</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cracking-the-interview/">Cracking the interview</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JVM/">JVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java-Project/">Java Project</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Leetcode/">Leetcode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PHP/">PHP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/http/">http</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/web/">web</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Nuage de mot-clés</h3>
    <div class="widget tagcloud">
      <a href="/tags/Big-Data-Process/" style="font-size: 15px;">Big Data Process</a> <a href="/tags/Cracking-the-Interview/" style="font-size: 17.5px;">Cracking the Interview</a> <a href="/tags/Cracking-the-interview/" style="font-size: 12.5px;">Cracking the interview</a> <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/JVM/" style="font-size: 12.5px;">JVM</a> <a href="/tags/Java-Project/" style="font-size: 12.5px;">Java Project</a> <a href="/tags/Leetcode/" style="font-size: 20px;">Leetcode</a> <a href="/tags/PHP/" style="font-size: 12.5px;">PHP</a> <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/http/" style="font-size: 12.5px;">http</a> <a href="/tags/web/" style="font-size: 10px;">web</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Articles récents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/03/24/Leetcode-042/">Leetcode-042</a>
          </li>
        
          <li>
            <a href="/2018/03/24/Leetcode-011/">Leetcode-011</a>
          </li>
        
          <li>
            <a href="/2018/03/22/proxy-server/">proxy_server</a>
          </li>
        
          <li>
            <a href="/2018/03/21/Leetcode-055/">Leetcode-055</a>
          </li>
        
          <li>
            <a href="/2018/03/21/Leetcode-045/">Leetcode-045</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 Mengzhuo You<br>
      Propulsé par <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>